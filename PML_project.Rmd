---
title: "Practical Machine Learning - Project Writeup"
output: 
  html_document:
    keep_md: true
---
##Summary
The following analysis uses data from individuals performing weight lifting exercises while wearing monitoring devices (accelerometers). These devices gathered information on how exactly these individuals completed the exercises, whether correctly or incorrectly. This analysis will attempt to classify each data record into the appropriate set {A, B, C, D, E} which corresponds to a correct or incorrect style of completing the weight lifting exercise.

##Data Cleansing and Preparation
The training data consists of 19,622 observations of 160 variables, and the testing set picks only 20 observations of the same variables. 

```{r, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(gbm)

pmltrain <- read.csv("pml-training.csv", na.strings = c("NA", ""))
pmltest <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
```

As the number of variables is very large, the results of my exploratory data analysis will be summarized rather than presented in detail. The most obvious problem is that there a large number of "NA" and "DIV/0" results in the data. These must addressed. It is important to note that the raw data was recorded over a period of time, and has been cut into a number of "windows" (information about these windows is recorded in the "New Window" and "Num Window" variables). After visually inspecting the raw data it became apparent that the NA and DIV/0 errors were confined to only some of the variables. Namely the variables that were summarizing the results of a given "window" -- the errors were confined to variables with names including: "var", "avg", "skewness", "kurtosis", "amplitude", "max", "min", and "stddev". These are all summary statistics (variance, average, etc) of groups of individual observations, and since I am interested in classifying individual records, summary data about groups of records is not helpful and can be removed from the data set.  
  
The first seven columns in the data set can also be removed as they are not useful to the classification process. For example, I assume the timestamp information does not impact what exercise group an individual was assigned. 

```{r, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
cleantrain <- pmltrain[, grep("var_|kurtosis_|avg_|skewness_|max_|min_|amplitude_|stddev_", 
                              names(pmltrain), value = TRUE, invert = TRUE)]
cleantest <- pmltest[, grep("var_|kurtosis_|avg_|skewness_|max_|min_|amplitude_|stddev_", 
                              names(pmltest), value = TRUE, invert = TRUE)]

##also need to remove the first seven columns of data - not relevant to the analysis at hand
cleantrain <- cleantrain[,-c(1:7)]
cleantest <- cleantest[,-c(1:7)]
```

After cleaning the data sets, there are 19,622 observations of 53 variables in the training set and 20 observations of the same variables in the test set.  

Using Principal Components Analysis we can get a visual sense of what the data may be able to tell us:
```{r, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
preProc <- preProcess(cleantrain[,-53], method = "pca", thres = 0.8)
trainPC <- predict(preProc, cleantrain[-53])
##summary(trainPC)
plot(trainPC[,1],trainPC[,2])
```

There are clearly five groups of observations in the data. Two of the groups may prove difficult to tell apart.  

However, PCA will be difficult to use for prediction because there are five categories to predict (linear regression models cannot be used), so a tree model approach should be appropriate. I will compare the Random Forest and GBM models.  


```{r, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
rfmodFit <- randomForest(x = cleantrain[,-53], y = cleantrain[,53], xtest = cleantest[,-53], importance = TRUE)
importance_vector <- as.data.frame(importance(rfmodFit, type = 2))
importance_vector
```
  
The variables "roll_belt" and "yaw_belt" provide the most explantory power to this model.  

##What is predicted?

```{r, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
rfmodFit$confusion

##give matrix of predictions
rfmodFit$test$votes
##manually pick predictions to submit to course website
##largest value in the votes matrix is the prediction
answers <- c("B","A","B","A","A","E","D","B","A","A","B","C","B","A","E","E","A","B","B","B")

## gbm - boosting - method ## ran once, and then commented out code to speed up report generation
##gbmFit <- train(classe ~ ., method = "gbm", data = cleantrain, verbose = FALSE)
##predict(gbmFit, cleantest[,-53])
## this method requires a lot of memory 

##pml_write_files = function(x){
##  n = length(x)
##  for(i in 1:n){
##    filename = paste0("problem_id_",i,".txt")
##    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
##  }
##}
##pml_write_files(answers)
```
##Results and Cross-Validation
The random forest model predicts these 20 results: "B","A","B","A","A","E","D","B","A","A","B","C","B","A","E","E","A","B","B","B".  

The GBM model also predicts the exact same results. Given the two models give the same results I expect the out of sample error to be quite low. The in-sample error from the random forest confusion matrix supports that expectation. The largest error shown is for classifying "D" and it is approximately 0.007 - this roughly equates to one classification error in 140 samples.  However, it is appropriate to also measure the out of sample error to obtain a less optimistic (or, more realistic) view of the limitations of the model. To do this, the training data set will be split into two parts - a new testing set will be created via random sampling, and the (reduced) training set will have those items removed. As the random forest model I use is very computationally intensive I am choosing to run only two cross-validation tests to conserve time.


```{r, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
##out of sample cross validation
set.seed(444) ## to ensure consistent selection of the test set
newtest1 <- cleantrain[sample(dim(cleantrain)[1], size = 100),]
newtrain1 <- cleantrain[-sample(dim(cleantrain)[1], size = 100),]

rfmodFit1 <- randomForest(x = newtrain1[,-53], y = newtrain1[,53], xtest = newtest1[,-53], 
                         ytest = newtest1[,53], importance = TRUE)

set.seed(1001) ## to ensure consistent selection of the test set
newtest2 <- cleantrain[sample(dim(cleantrain)[1], size = 100),]
newtrain2 <- cleantrain[-sample(dim(cleantrain)[1], size = 100),]

rfmodFit2 <- randomForest(x = newtrain2[,-53], y = newtrain2[,53], xtest = newtest2[,-53],
                          ytest = newtest2[,53], importance = TRUE)

rfmodFit1$confusion
rfmodFit2$confusion
```

Both out of sample errors are similar to those found for the in-sample errors. As the data set is quite large, and the groupings of the category variables are quite distinct, it is likely that to see material out of sample error we would need to reduce the size of the training set rather dramatically.  

##Conclusion
Both the Random Forest and GBM models predict exactly the same classifications for the testing set data. Those results were submitted to the Coursera website for validation. Given the low expected (in-sample and out-of-sample) classification error, it is expected that at most only one of the results will be incorrect (though even one error should be unlikely). The final tally indeed was 100% accuracy. We would expect to see some errors if a larger testing set were used.
